# Services - æ¥­å‹™é‚è¼¯æœå‹™å±¤

æœ¬ç›®éŒ„åŒ…å«æ‰€æœ‰æ ¸å¿ƒæ¥­å‹™é‚è¼¯æœå‹™çš„å¯¦ç¾ï¼Œæä¾›å‘é‡å„²å­˜ã€æª”æ¡ˆç´¢å¼•ã€åµŒå…¥ç”Ÿæˆç­‰åŠŸèƒ½ã€‚æ¯å€‹æœå‹™éƒ½æ¡ç”¨å–®ä¸€è·è²¬åŸå‰‡ï¼Œä¾¿æ–¼ç¶­è­·å’Œæ“´å±•ã€‚

## ğŸ“‚ æª”æ¡ˆæ¦‚è¿°

### `vector_store_service.py`
è² è²¬å‘é‡è³‡æ–™åº«ç®¡ç†ã€èªç¾©æœå°‹å’ŒçŸ¥è­˜å„²å­˜æ“ä½œã€‚

### `context_chunking_service.py`
è² è²¬æ‰¹é‡æª”æ¡ˆç´¢å¼•ã€åˆ†å¡Šå’Œæ™ºèƒ½æ–‡ä»¶æƒæã€‚

---

## ğŸ—ï¸ æœå‹™æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Controllers Layer                  â”‚
â”‚  (MCP Tools å…¥å£)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Services Layer (æœ¬ç›®éŒ„)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ VectorStoreService    (å‘é‡å„²å­˜å’Œæœå°‹)   â”‚
â”‚ â€¢ ContextChunkingService (æ‰¹é‡ç´¢å¼•)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Storage & External Layer           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ ChromaDB          (å‘é‡è³‡æ–™åº«)     â”‚
â”‚ â€¢ SentenceTransformer (æ–‡æœ¬åµŒå…¥æ¨¡å‹)  â”‚
â”‚ â€¢ MarkdownParser    (æ–‡æª”è§£æ)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š VectorStoreService

è™•ç†æ‰€æœ‰å‘é‡å„²å­˜ã€åµŒå…¥ç”Ÿæˆå’Œèªç¾©æœå°‹ç›¸é—œæ“ä½œçš„æ ¸å¿ƒæœå‹™ã€‚

### ä¸»è¦è·è²¬

| è·è²¬ | èªªæ˜ | ç›¸é—œæ–¹æ³• |
|------|------|---------|
| **çŸ¥è­˜å„²å­˜** | å°‡æ–‡æœ¬è½‰æ›ç‚ºå‘é‡ä¸¦å„²å­˜åˆ° ChromaDB | `add_knowledge()` |
| **èªç¾©æœå°‹** | åŸ·è¡Œå‘é‡ç›¸ä¼¼åº¦æœå°‹ï¼Œè¿”å›ç›¸é—œçŸ¥è­˜ | `search_knowledge()` |
| **æª”æ¡ˆå„²å­˜** | è§£æä¸¦å„²å­˜ Markdown/JSON/TXT æª”æ¡ˆ | `store_document()` |
| **ä¸»é¡Œæª¢ç´¢** | æŒ‰ä¸»é¡Œæª¢ç´¢æ‰€æœ‰çŸ¥è­˜é» | `get_all_by_topic()` |
| **åµŒå…¥ç”Ÿæˆ** | ä½¿ç”¨ SentenceTransformer ç”Ÿæˆæ–‡æœ¬åµŒå…¥ | å…§éƒ¨ä½¿ç”¨ |
| **ç¨‹å¼ç¢¼åˆ†é›¢** | v2.0 ç‰¹æ€§ï¼šåˆ†é›¢ç¨‹å¼ç¢¼å’Œæ–‡å­—å…§å®¹ | `_extract_code_blocks()` |

### åˆå§‹åŒ–

```python
from services.vector_store_service import VectorStoreService

# ä½¿ç”¨é è¨­é…ç½®
vector_store = VectorStoreService(
    db_path="./chroma_db",                              # ChromaDB å„²å­˜è·¯å¾‘
    collection_name="ai_documentation",                 # é›†åˆåç¨±
    embedding_model="paraphrase-multilingual-MiniLM-L12-v2"  # åµŒå…¥æ¨¡å‹
)
```

**åƒæ•¸èªªæ˜ï¼š**

| åƒæ•¸ | é¡å‹ | é è¨­å€¼ | èªªæ˜ |
|------|------|--------|------|
| `db_path` | `str` | `"./chroma_db"` | ChromaDB æ•¸æ“šåº«å„²å­˜ç›®éŒ„ |
| `collection_name` | `str` | `"mcp_knowledge_base"` | ä½¿ç”¨çš„é›†åˆåç¨±ï¼ˆæ”¯æ´å¤šå€‹é›†åˆï¼‰ |
| `embedding_model` | `str` | `"paraphrase-multilingual-MiniLM-L12-v2"` | åµŒå…¥æ¨¡å‹ï¼ˆæ”¯æ´å¤šèªè¨€ï¼‰ |

### æ ¸å¿ƒæ–¹æ³•

#### 1. `add_knowledge(topic: str, content: str) -> str`

**åŠŸèƒ½ï¼š** æ·»åŠ æ–°çŸ¥è­˜é»åˆ°å‘é‡è³‡æ–™åº«ã€‚

**åƒæ•¸ï¼š**
- `topic` (str)ï¼šçŸ¥è­˜é»ä¸»é¡Œåˆ†é¡
- `content` (str)ï¼šçŸ¥è­˜é»æ–‡æœ¬å…§å®¹

**è¿”å›å€¼ï¼š** UUID å­—ç¬¦ä¸²ï¼Œä½œç‚ºçŸ¥è­˜é»çš„å”¯ä¸€è­˜åˆ¥ç¬¦

**å¯¦ç¾æµç¨‹ï¼š**
1. ç”Ÿæˆ UUID ä½œç‚ºæ–‡æª” ID
2. ä½¿ç”¨ SentenceTransformer è¨ˆç®— embedding
3. å„²å­˜åˆ° ChromaDB çš„ç•¶å‰é›†åˆ

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
doc_id = vector_store.add_knowledge(
    topic="DDD",
    content="Aggregate æ˜¯é ˜åŸŸé©…å‹•è¨­è¨ˆä¸­çš„æ ¸å¿ƒæ¦‚å¿µ..."
)
print(f"Stored with ID: {doc_id}")
```

---

#### 2. `search_knowledge(query: str, top_k: int = 20, topic: Optional[str] = None) -> List[KnowledgePoint]`

**åŠŸèƒ½ï¼š** åŸ·è¡Œèªç¾©æœå°‹ï¼Œè¿”å›æœ€ç›¸é—œçš„çŸ¥è­˜é»ã€‚

**åƒæ•¸ï¼š**
- `query` (str)ï¼šè‡ªç„¶èªè¨€æœå°‹æŸ¥è©¢
- `top_k` (int, é è¨­=20)ï¼šè¿”å›çµæœæ•¸é‡
- `topic` (str, å¯é¸)ï¼šæŒ‰ä¸»é¡Œç¯©é¸

**è¿”å›å€¼ï¼š** `KnowledgePoint` ç‰©ä»¶åˆ—è¡¨ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰

**æœå°‹æµç¨‹ï¼š**
1. è¨ˆç®—æŸ¥è©¢çš„ embedding å‘é‡
2. åœ¨ ChromaDB ä¸­åŸ·è¡Œ cosine similarity æœå°‹
3. ååºåˆ—åŒ– code_blocks å…ƒæ•¸æ“š
4. è¿”å› KnowledgePoint ç‰©ä»¶ï¼ˆåŒ…å«ç›¸ä¼¼åº¦åˆ†æ•¸ï¼‰

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
# å…¨åŸŸæœå°‹
results = vector_store.search_knowledge(
    query="å¦‚ä½•å¯¦ç¾ DDD ä¸­çš„ Aggregate",
    top_k=5
)

# æŒ‰ä¸»é¡Œæœå°‹
results = vector_store.search_knowledge(
    query="Event Sourcing",
    topic="EventSourcing",
    top_k=3
)

# éæ­·çµæœ
for point in results:
    print(f"ç›¸ä¼¼åº¦: {point.similarity:.2f}")
    print(f"ä¸»é¡Œ: {point.topic}")
    print(f"å…§å®¹: {point.content[:200]}...")
    if point.code_blocks:
        print(f"ç¨‹å¼ç¢¼ç¯„ä¾‹: {len(point.code_blocks)} å€‹")
```

**æ•ˆèƒ½ç‰¹æ€§ï¼š**
- æœå°‹å»¶é²ï¼š< 100msï¼ˆ1000 æ–‡æª”å…§ï¼‰
- åµŒå…¥è¨ˆç®—ï¼š~1000 tokens/sec
- è¨˜æ†¶é«”ä½¿ç”¨ï¼šæ¨¡å‹è¼‰å…¥ ~200MB

---

#### 3. `store_document(file_path: str, topic: Optional[str] = None) -> str`

**åŠŸèƒ½ï¼š** è®€å–æª”æ¡ˆä¸¦å„²å­˜åˆ°å‘é‡è³‡æ–™åº«ã€‚

**åƒæ•¸ï¼š**
- `file_path` (str)ï¼šæª”æ¡ˆçš„çµ•å°æˆ–ç›¸å°è·¯å¾‘
- `topic` (str, å¯é¸)ï¼šä¸»é¡Œåˆ†é¡ï¼ˆé è¨­ä½¿ç”¨æª”åï¼‰

**æ”¯æ´æ ¼å¼ï¼š** `.md`, `.txt`, `.json`

**è¿”å›å€¼ï¼š** ç¢ºèªè¨Šæ¯ï¼ˆå­—ç¬¦ä¸²ï¼‰

**è™•ç†æµç¨‹ï¼š**
1. æª¢é©—æª”æ¡ˆå­˜åœ¨æ€§å’Œé¡å‹
2. è®€å–æª”æ¡ˆå…§å®¹
3. ä½¿ç”¨ MarkdownParser æå–æ–‡æœ¬å’Œç¨‹å¼ç¢¼
4. é€²è¡Œæ™ºèƒ½åˆ†å¡Šï¼ˆchunkï¼‰
5. é€å€‹ chunk è¨ˆç®— embedding ä¸¦å„²å­˜

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
# ä½¿ç”¨æª”åä½œç‚º topic
result = vector_store.store_document("./docs/ARCHITECTURE.md")

# æŒ‡å®šè‡ªè¨‚ topic
result = vector_store.store_document(
    "./docs/spec.md",
    topic="Specification"
)
```

---

#### 4. `get_all_by_topic(topic: str) -> List[KnowledgePoint]`

**åŠŸèƒ½ï¼š** æª¢ç´¢ç‰¹å®šä¸»é¡Œçš„æ‰€æœ‰çŸ¥è­˜é»ã€‚

**åƒæ•¸ï¼š**
- `topic` (str)ï¼šä¸»é¡Œåç¨±

**è¿”å›å€¼ï¼š** è©²ä¸»é¡Œçš„æ‰€æœ‰ KnowledgePoint ç‰©ä»¶

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
# ç²å–æ‰€æœ‰ DDD ç›¸é—œçŸ¥è­˜
ddd_knowledge = vector_store.get_all_by_topic("DDD")
print(f"Found {len(ddd_knowledge)} DDD knowledge points")

# é¡¯ç¤ºä¸»é¡Œä¸‹çš„æ‰€æœ‰çŸ¥è­˜
for point in ddd_knowledge:
    print(f"- {point.section_title} (ID: {point.id})")
```

---

### å…§éƒ¨å¯¦ç¾ç´°ç¯€

#### ç¨‹å¼ç¢¼åˆ†é›¢ (v2.0 ç‰¹æ€§)

ç³»çµ±ä½¿ç”¨æ™ºèƒ½ç¨‹å¼ç¢¼åˆ†é›¢ä»¥æå‡æœå°‹å“è³ªï¼š

```python
# å„²å­˜æ™‚ï¼š
åŸå§‹æ–‡æœ¬ (å«ç¨‹å¼ç¢¼)
  â†“
MarkdownParser.extract_code_blocks()
  â†“
(text_only, code_blocks)
  â†“
embedding = model.encode(text_only)  # åªå°æ–‡å­—è¨ˆç®—
  â†“
ChromaDB.add(
    embeddings=[embedding],
    documents=[text_only],
    metadatas={
        code_blocks: JSON.stringify(code_blocks)
    }
)
```

**æ•ˆèƒ½æ”¹å–„ï¼š**
- åµŒå…¥å¤§å°æ¸›å°‘ï¼š61-68%
- èªæ„ç²¾æº–åº¦æå‡ï¼š~40%
- æœå°‹é€Ÿåº¦æå‡ï¼šæ›´å°çš„å‘é‡é‹ç®—

---

#### ChromaDB é›†åˆç®¡ç†

æ”¯æ´å¤šå€‹é›†åˆä»¥çµ„ç¹”ä¸åŒé¡å‹çš„çŸ¥è­˜ï¼š

```python
# é›†åˆ 1ï¼šAI æ–‡æª”ç´¢å¼•
vector_store_ai = VectorStoreService(
    collection_name="ai_documentation"  # 1,116 chunks
)

# é›†åˆ 2ï¼šæ‰‹å‹•çŸ¥è­˜åº«
vector_store_kb = VectorStoreService(
    collection_name="mcp_knowledge_base"  # 2+ æ–‡æª”
)
```

---

## ğŸ“‹ ContextChunkingService

è™•ç†æ‰¹é‡æª”æ¡ˆç´¢å¼•å’Œæ™ºèƒ½åˆ†å¡Šçš„æœå‹™ã€‚

### ä¸»è¦è·è²¬

| è·è²¬ | èªªæ˜ | ç›¸é—œæ–¹æ³• |
|------|------|---------|
| **ç›®éŒ„æƒæ** | éæ­¸æƒæè³‡æ–™å¤¾å°‹æ‰¾æ”¯æ´çš„æª”æ¡ˆ | `scan_directory()` |
| **æª”æ¡ˆéæ¿¾** | æŒ‰å‰¯æª”åç¯©é¸æª”æ¡ˆ | `scan_directory()` |
| **å…ƒæ•¸æ“šæå–** | å¾æª”æ¡ˆè·¯å¾‘æå–çµæ§‹åŒ–å…ƒæ•¸æ“š | `extract_metadata()` |
| **æ‰¹é‡ç´¢å¼•** | æ‰¹é‡è™•ç†å’Œç´¢å¼•æª”æ¡ˆ | `chunk_folder()` |
| **çµ±è¨ˆæ”¶é›†** | æ”¶é›†ç´¢å¼•æ“ä½œçš„çµ±è¨ˆè³‡è¨Š | `chunk_folder()` |

### åˆå§‹åŒ–

```python
from services.context_chunking_service import ContextChunkingService

chunking_service = ContextChunkingService(vector_store=vector_store)
```

### æ ¸å¿ƒæ–¹æ³•

#### 1. `scan_directory(source_dir: str, file_extensions: Optional[Set[str]] = None) -> List[Path]`

**åŠŸèƒ½ï¼š** éæ­¸æƒæç›®éŒ„å°‹æ‰¾æŒ‡å®šé¡å‹çš„æª”æ¡ˆã€‚

**åƒæ•¸ï¼š**
- `source_dir` (str)ï¼šè¦æƒæçš„ç›®éŒ„è·¯å¾‘
- `file_extensions` (Optional[Set[str]])ï¼šè¦åŒ…å«çš„æª”æ¡ˆå‰¯æª”å

**é è¨­æ”¯æ´é¡å‹ï¼š**
```python
{'.md', '.txt', '.java', '.py', '.js', '.ts', '.sh', '.json', '.yaml', '.yml'}
```

**è¿”å›å€¼ï¼š** ç¬¦åˆæ¢ä»¶çš„æª”æ¡ˆè·¯å¾‘åˆ—è¡¨

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
# æƒææ‰€æœ‰æ”¯æ´çš„æª”æ¡ˆ
files = chunking_service.scan_directory("./documentation")

# åªæƒæ Markdown æª”æ¡ˆ
files = chunking_service.scan_directory(
    "./docs",
    file_extensions={'.md', '.txt'}
)

print(f"Found {len(files)} files")
```

---

#### 2. `extract_metadata(file_path: Path, source_dir: Path) -> Dict[str, Any]`

**åŠŸèƒ½ï¼š** å¾æª”æ¡ˆè·¯å¾‘æå–çµæ§‹åŒ–å…ƒæ•¸æ“šã€‚

**åƒæ•¸ï¼š**
- `file_path` (Path)ï¼šæª”æ¡ˆçš„å®Œæ•´è·¯å¾‘
- `source_dir` (Path)ï¼šæ ¹ç›®éŒ„è·¯å¾‘

**è¿”å›å€¼ï¼š** åŒ…å«ä»¥ä¸‹æ¬„ä½çš„å­—å…¸ï¼š
```python
{
    "file_path": str,          # å®Œæ•´æª”æ¡ˆè·¯å¾‘
    "relative_path": str,      # ç›¸å°æ–¼æ ¹ç›®éŒ„çš„è·¯å¾‘
    "file_name": str,          # æª”æ¡ˆåç¨±
    "category": str,           # ç›®éŒ„åˆ†é¡
    "topic": str,              # ä¸»é¡Œï¼ˆä¾†è‡ªæª”åï¼‰
    "file_size": int           # æª”æ¡ˆå¤§å°ï¼ˆä½å…ƒçµ„ï¼‰
}
```

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
from pathlib import Path

metadata = chunking_service.extract_metadata(
    file_path=Path("./docs/architecture/ARCHITECTURE.md"),
    source_dir=Path("./docs")
)
# çµæœï¼š
# {
#     "file_path": "/full/path/to/ARCHITECTURE.md",
#     "relative_path": "architecture/ARCHITECTURE.md",
#     "file_name": "ARCHITECTURE.md",
#     "category": "architecture",
#     "topic": "ARCHITECTURE",
#     "file_size": 15234
# }
```

---

#### 3. `chunk_folder(source_dir: str, chunk_size: int = 4000, chunk_overlap: int = 200, file_extensions: Optional[List[str]] = None) -> IndexingStats`

**åŠŸèƒ½ï¼š** æ‰¹é‡ç´¢å¼•è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰æª”æ¡ˆã€‚

**åƒæ•¸ï¼š**
- `source_dir` (str)ï¼šè¦ç´¢å¼•çš„è³‡æ–™å¤¾è·¯å¾‘
- `chunk_size` (int, é è¨­=4000)ï¼šæ¯å€‹ chunk çš„æœ€å¤§å­—å…ƒæ•¸
- `chunk_overlap` (int, é è¨­=200)ï¼šç›¸é„° chunk ä¹‹é–“çš„é‡ç–Šå­—å…ƒæ•¸
- `file_extensions` (Optional[List[str]])ï¼šè¦è™•ç†çš„å‰¯æª”å

**è¿”å›å€¼ï¼š** `IndexingStats` ç‰©ä»¶ï¼ŒåŒ…å«ï¼š
```python
{
    "total_files": int,              # æƒæåˆ°çš„æª”æ¡ˆç¸½æ•¸
    "processed_files": int,          # æˆåŠŸè™•ç†çš„æª”æ¡ˆæ•¸
    "failed_files": int,             # å¤±æ•—çš„æª”æ¡ˆæ•¸
    "total_chunks": int,             # ç”Ÿæˆçš„ chunk ç¸½æ•¸
    "duration_seconds": float,       # ç¸½è€—æ™‚
    "file_details": List[Dict]       # æ¯å€‹æª”æ¡ˆçš„è©³ç´°è³‡è¨Š
}
```

**ç´¢å¼•æµç¨‹ï¼š**
1. æƒæç›®éŒ„å°‹æ‰¾ç¬¦åˆæ¢ä»¶çš„æª”æ¡ˆ
2. ä¾æ¬¡è™•ç†æ¯å€‹æª”æ¡ˆ
3. é€²è¡Œæ™ºèƒ½åˆ†å¡Šï¼ˆæ··åˆç­–ç•¥ï¼‰
4. è¨ˆç®— embedding ä¸¦å„²å­˜
5. æ”¶é›†çµ±è¨ˆè³‡è¨Šä¸¦è¿”å›

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
# åŸºæœ¬ç”¨æ³•
stats = chunking_service.chunk_folder("./documentation")
print(f"Processed {stats.processed_files}/{stats.total_files} files")
print(f"Generated {stats.total_chunks} chunks in {stats.duration_seconds:.2f}s")

# è‡ªè¨‚é…ç½®
stats = chunking_service.chunk_folder(
    source_dir="./docs",
    chunk_size=6000,
    chunk_overlap=300,
    file_extensions=[".md", ".txt"]
)

# æŸ¥çœ‹è©³ç´°è³‡è¨Š
for file_info in stats.file_details:
    if file_info["status"] == "success":
        print(f"âœ“ {file_info['file']}: {file_info['chunks']} chunks")
    else:
        print(f"âœ— {file_info['file']}: {file_info.get('error', 'Unknown error')}")
```

---

## ğŸ”„ è³‡æ–™æµç¨‹

### çŸ¥è­˜å„²å­˜æµç¨‹

```
User æä¾›æ–‡æœ¬æˆ–æª”æ¡ˆ
    â†“
Controllers (çŸ¥è­˜å…¥å£)
    â†“
VectorStoreService.add_knowledge()
    æˆ– .store_document()
    â†“
MarkdownParser.extract_code_blocks() [v2.0]
    â†“
(text_only, code_blocks)
    â†“
SentenceTransformer.encode(text_only)
    â†“
ChromaDB.add(
    embeddings=[å‘é‡],
    documents=[æ–‡æœ¬],
    metadatas={topic, code_blocks, ...}
)
    â†“
è¿”å› Document ID çµ¦ User
```

### èªç¾©æœå°‹æµç¨‹

```
User æä¾›æœå°‹æŸ¥è©¢
    â†“
Controllers (æœå°‹å…¥å£)
    â†“
VectorStoreService.search_knowledge()
    â†“
SentenceTransformer.encode(query)
    â†“
ChromaDB.query(
    query_embeddings=[å‘é‡],
    n_results=top_k,
    where={topic: ...}  # å¯é¸ç¯©é¸
)
    â†“
ååºåˆ—åŒ– code_blocks å…ƒæ•¸æ“š
    â†“
List[KnowledgePoint] (æŒ‰ç›¸ä¼¼åº¦æ’åº)
    â†“
Controllers åŒ…è£ç‚º SearchResult
    â†“
è¿”å›çµ¦ Claude CLI
```

### æ‰¹é‡ç´¢å¼•æµç¨‹

```
User æŒ‡å®šè³‡æ–™å¤¾
    â†“
ContextChunkingService.chunk_folder()
    â†“
scan_directory()
    â†“
[file1, file2, file3, ...]
    â†“
For each file:
  â”œâ”€ extract_metadata()
  â”œâ”€ read file content
  â”œâ”€ intelligent chunking
  â”œâ”€ VectorStoreService.store_document()
  â”‚   â”œâ”€ MarkdownParser
  â”‚   â”œâ”€ SentenceTransformer.encode()
  â”‚   â””â”€ ChromaDB.add()
  â””â”€ track statistics
    â†“
IndexingStats {
    processed_files,
    total_chunks,
    duration_seconds,
    file_details
}
    â†“
è¿”å›çµ±è¨ˆè³‡è¨Šçµ¦ User
```

---

## ğŸ¯ è¨­è¨ˆåŸå‰‡

### 1. å–®ä¸€è·è²¬
- **VectorStoreService**ï¼šåªè² è²¬å‘é‡å„²å­˜å’Œæœå°‹
- **ContextChunkingService**ï¼šåªè² è²¬æª”æ¡ˆæƒæå’Œç´¢å¼•å”èª¿
- å…¶ä»–è·è²¬å§”æ´¾çµ¦å°ˆé–€çš„å·¥å…·ï¼ˆMarkdownParserã€SentenceTransformerï¼‰

### 2. ä¾è³´æ³¨å…¥
```python
# è‰¯å¥½å¯¦è¸ï¼šä¾è³´ä½œç‚ºåƒæ•¸å‚³å…¥
chunking_service = ContextChunkingService(vector_store=vector_store)

# è€Œä¸æ˜¯å…§éƒ¨å»ºç«‹
# chunking_service = ContextChunkingService()
# chunking_service.vector_store = VectorStoreService()  # ä¸å¥½
```

### 3. å¯æ¸¬è©¦æ€§
- æ‰€æœ‰æ–¹æ³•éƒ½æ˜¯ç´”å‡½æ•¸ï¼ˆç„¡å‰¯ä½œç”¨ï¼‰
- æ”¯æ´ mock VectorStoreService é€²è¡Œå–®å…ƒæ¸¬è©¦
- æ”¯æ´æ¸¬è©¦è³‡æ–™åº«è·¯å¾‘

### 4. å¯æ“´å±•æ€§
- æ”¯æ´è‡ªè¨‚åµŒå…¥æ¨¡å‹
- æ”¯æ´å¤šå€‹ ChromaDB é›†åˆ
- æ”¯æ´è‡ªè¨‚æª”æ¡ˆå‰¯æª”åéæ¿¾

---

## ğŸ’¡ å¸¸è¦‹ä½¿ç”¨å ´æ™¯

### å ´æ™¯ 1ï¼šå»ºç«‹æ–°çš„çŸ¥è­˜åº«

```python
# åˆå§‹åŒ–
vs = VectorStoreService(
    db_path="./project_kb",
    collection_name="my_project"
)

# æ·»åŠ å–®å€‹çŸ¥è­˜é»
doc_id = vs.add_knowledge(
    topic="Architecture",
    content="ç³»çµ±æ¡ç”¨å¾®æœå‹™æ¶æ§‹..."
)

# æˆ–å„²å­˜æ•´å€‹æª”æ¡ˆ
vs.store_document("./docs/ARCHITECTURE.md")
```

### å ´æ™¯ 2ï¼šæ‰¹é‡ç´¢å¼•ç¾æœ‰æ–‡æª”

```python
chunking = ContextChunkingService(vs)

# ç´¢å¼•æ•´å€‹æ–‡æª”ç›®éŒ„
stats = chunking.chunk_folder(
    source_dir="./project_documentation",
    chunk_size=4000,
    chunk_overlap=200
)

print(f"Successfully indexed {stats.processed_files} files")
print(f"Generated {stats.total_chunks} searchable chunks")
```

### å ´æ™¯ 3ï¼šå¯¦ç¾æ™ºèƒ½æœå°‹

```python
# å…¨åŸŸæœå°‹
results = vs.search_knowledge(
    query="å¦‚ä½•é…ç½®å¾®æœå‹™é€šä¿¡ï¼Ÿ",
    top_k=5
)

# æ‰“å°çµæœ
for point in results:
    print(f"[{point.similarity:.2f}] {point.section_title}")
    print(f"    {point.content[:100]}...")
    if point.code_blocks:
        for cb in point.code_blocks:
            print(f"    Code: {cb.language}")
```

### å ´æ™¯ 4ï¼šæŒ‰ä¸»é¡Œæª¢ç´¢

```python
# ç²å–æ‰€æœ‰æ¶æ§‹ç›¸é—œæ–‡æª”
architecture_docs = vs.get_all_by_topic("Architecture")

for doc in architecture_docs:
    print(f"- {doc.file_path}")
    print(f"  Section: {doc.section_title}")
```

---

## ğŸ”§ æ•ˆèƒ½æœ€ä½³å¯¦è¸

1. **æ‰¹é‡æ“ä½œ**
   - ä½¿ç”¨ `chunk_folder()` è€Œéå¤šæ¬¡ `add_knowledge()`
   - æ¸›å°‘ API å‘¼å«æ¬¡æ•¸

2. **Top-K é™åˆ¶**
   - æœå°‹æ™‚é™åˆ¶ `top_k <= 20`
   - æ¸›å°‘å‚³è¼¸æ•¸æ“šé‡å’Œè¨ˆç®—æ™‚é–“

3. **é›†åˆéš”é›¢**
   - ç‚ºä¸åŒé¡å‹çŸ¥è­˜ä½¿ç”¨ä¸åŒé›†åˆ
   - æå‡æœå°‹æ•ˆç‡

4. **æ¨¡å‹é¸æ“‡**
   - é è¨­æ¨¡å‹æ”¯æ´ 50+ èªè¨€
   - å°æ–¼ç‰¹æ®Šå ´æ™¯å¯è€ƒæ…®æ›´å°ˆé–€çš„æ¨¡å‹

---

## ğŸ“š ç›¸é—œæ–‡ä»¶

- [models/README.md](../models/README.md) - è³‡æ–™æ¨¡å‹å®šç¾©
- [controllers/README.md](../controllers/README.md) - MCP å·¥å…·å±¤
- [CODE_SEPARATION.md](../docs/CODE_SEPARATION.md) - v2.0 æŠ€è¡“ç´°ç¯€
- [MCP_SERVER_CONFIG.md](../MCP_SERVER_CONFIG.md) - ä¼ºæœå™¨é…ç½®

---

**æœ€å¾Œæ›´æ–°ï¼š** 2025-11-24
**ç‰ˆæœ¬ï¼š** v2.0
**ç¶­è­·è€…ï¼š** RAG Memory MCP Team

