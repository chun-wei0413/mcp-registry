#!/usr/bin/env python3
"""
é©—è­‰ .ai ç›®éŒ„æ–‡æª” Embedding v2.0 - æ™ºèƒ½ç¨‹å¼ç¢¼åˆ†é›¢

é©—è­‰é …ç›®ï¼š
1. ChromaDB é›†åˆåŒ…å«çš„æ–‡æª”æ•¸é‡å’Œåˆ†ä½ˆ
2. ç¨‹å¼ç¢¼å¡Šæ˜¯å¦æ­£ç¢ºåˆ†é›¢ä¸¦å„²å­˜
3. å…ƒæ•¸æ“šæ˜¯å¦å®Œæ•´
4. æœå°‹åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
5. èˆ‡ v1.0 ç‰ˆæœ¬çš„æ¯”è¼ƒï¼ˆå¦‚æœæœ‰ï¼‰
"""

import json
from pathlib import Path
from typing import Dict, List, Any
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from services.vector_store_service import VectorStoreService


class AIDocsV2Verifier:
    """é©—è­‰å™¨"""

    def __init__(self, vector_store: VectorStoreService):
        self.vector_store = vector_store
        self.separator_line = "=" * 80

    def run_verification(self) -> Dict[str, Any]:
        """é‹è¡Œæ‰€æœ‰é©—è­‰"""
        results = {
            'timestamp': self._get_timestamp(),
            'sections': {}
        }

        print(f"\n{self.separator_line}")
        print("é©—è­‰ .ai ç›®éŒ„æ–‡æª” Embedding v2.0 - æ™ºèƒ½ç¨‹å¼ç¢¼åˆ†é›¢".center(80))
        print(self.separator_line + "\n")

        # 1. é›†åˆçµ±è¨ˆ
        results['sections']['collection_stats'] = self._verify_collection_stats()

        # 2. å…ƒæ•¸æ“šé©—è­‰
        results['sections']['metadata_validation'] = self._verify_metadata()

        # 3. ç¨‹å¼ç¢¼åˆ†é›¢é©—è­‰
        results['sections']['code_separation'] = self._verify_code_separation()

        # 4. æœå°‹åŠŸèƒ½é©—è­‰
        results['sections']['search_validation'] = self._verify_search()

        # 5. æ€§èƒ½æŒ‡æ¨™
        results['sections']['performance_metrics'] = self._analyze_performance()

        # 6. å»ºè­°
        results['recommendations'] = self._generate_recommendations(results)

        return results

    def _verify_collection_stats(self) -> Dict[str, Any]:
        """é©—è­‰é›†åˆçµ±è¨ˆè³‡è¨Š"""
        print("\n[1] é›†åˆçµ±è¨ˆè³‡è¨Š")
        print("-" * 80)

        collection = self.vector_store.collection
        count = collection.count()

        stats = {
            'total_documents': count,
            'status': 'OK' if count > 0 else 'ERROR'
        }

        print(f"ç¸½æ–‡æª”æ•¸: {count}")

        # å–æ¨£æª¢æŸ¥æ–‡æª”
        if count > 0:
            sample = collection.get(limit=5)
            print(f"\næ¨£æœ¬æ–‡æª”ï¼ˆå‰ 5 å€‹ï¼‰:")

            for i, doc_id in enumerate(sample['ids'][:5], 1):
                idx = sample['ids'].index(doc_id)
                metadata = sample['metadatas'][idx]
                content_preview = sample['documents'][idx][:50].replace('\n', ' ')

                print(f"\n  {i}. ID: {doc_id[:8]}...")
                print(f"     Category: {metadata.get('category', 'N/A')}")
                print(f"     Priority: {metadata.get('priority', 'N/A')}")
                print(f"     Topics: {metadata.get('topics', 'N/A')}")
                print(f"     Code Blocks: {metadata.get('code_block_count', 0)}")
                print(f"     Preview: {content_preview}...")

        return stats

    def _verify_metadata(self) -> Dict[str, Any]:
        """é©—è­‰å…ƒæ•¸æ“šå®Œæ•´æ€§"""
        print(f"\n[2] å…ƒæ•¸æ“šé©—è­‰")
        print("-" * 80)

        collection = self.vector_store.collection
        all_docs = collection.get()

        metadata_stats = {
            'total_documents': len(all_docs['ids']),
            'with_code_blocks': 0,
            'by_priority': {},
            'by_category': {},
            'missing_fields': {
                'source_file': 0,
                'category': 0,
                'priority': 0,
                'topics': 0,
            }
        }

        for i, doc_id in enumerate(all_docs['ids']):
            metadata = all_docs['metadatas'][i]

            # æª¢æŸ¥ç¨‹å¼ç¢¼å¡Š
            if int(metadata.get('code_block_count', 0)) > 0:
                metadata_stats['with_code_blocks'] += 1

            # çµ±è¨ˆå„ªå…ˆç´š
            priority = metadata.get('priority', 'unknown')
            metadata_stats['by_priority'][priority] = metadata_stats['by_priority'].get(priority, 0) + 1

            # çµ±è¨ˆåˆ†é¡
            category = metadata.get('category', 'unknown')
            metadata_stats['by_category'][category] = metadata_stats['by_category'].get(category, 0) + 1

            # æª¢æŸ¥å¿…è¦æ¬„ä½
            if not metadata.get('source_file'):
                metadata_stats['missing_fields']['source_file'] += 1
            if not metadata.get('category'):
                metadata_stats['missing_fields']['category'] += 1
            if not metadata.get('priority'):
                metadata_stats['missing_fields']['priority'] += 1

        print(f"åŒ…å«ç¨‹å¼ç¢¼å¡Šçš„æ–‡æª”: {metadata_stats['with_code_blocks']} / {metadata_stats['total_documents']}")
        print(f"ç¨‹å¼ç¢¼å¡Šè¦†è“‹ç‡: {metadata_stats['with_code_blocks']/metadata_stats['total_documents']*100:.1f}%")

        print(f"\nå„ªå…ˆç´šåˆ†ä½ˆ:")
        for priority in ['critical', 'high', 'medium', 'low']:
            count = metadata_stats['by_priority'].get(priority, 0)
            pct = count / metadata_stats['total_documents'] * 100 if metadata_stats['total_documents'] > 0 else 0
            print(f"  - {priority}: {count} ({pct:.1f}%)")

        print(f"\nä¸»è¦åˆ†é¡ï¼ˆå‰ 10 å€‹ï¼‰:")
        sorted_categories = sorted(metadata_stats['by_category'].items(), key=lambda x: x[1], reverse=True)
        for category, count in sorted_categories[:10]:
            pct = count / metadata_stats['total_documents'] * 100
            print(f"  - {category}: {count} ({pct:.1f}%)")

        print(f"\néºæ¼çš„å¿…è¦æ¬„ä½:")
        for field, missing_count in metadata_stats['missing_fields'].items():
            if missing_count > 0:
                print(f"  âš ï¸  {field}: {missing_count} å€‹æ–‡æª”éºæ¼")

        return metadata_stats

    def _verify_code_separation(self) -> Dict[str, Any]:
        """é©—è­‰ç¨‹å¼ç¢¼åˆ†é›¢åŠŸèƒ½"""
        print(f"\n[3] ç¨‹å¼ç¢¼åˆ†é›¢é©—è­‰")
        print("-" * 80)

        collection = self.vector_store.collection
        all_docs = collection.get()

        separation_stats = {
            'total_documents': len(all_docs['ids']),
            'documents_with_code_blocks': 0,
            'total_code_blocks': 0,
            'avg_code_blocks_per_doc': 0.0,
            'max_code_blocks_in_doc': 0,
            'samples': []
        }

        for i, doc_id in enumerate(all_docs['ids']):
            metadata = all_docs['metadatas'][i]
            code_block_count = int(metadata.get('code_block_count', 0))

            if code_block_count > 0:
                separation_stats['documents_with_code_blocks'] += 1
                separation_stats['total_code_blocks'] += code_block_count

                if code_block_count > separation_stats['max_code_blocks_in_doc']:
                    separation_stats['max_code_blocks_in_doc'] = code_block_count

                # æ”¶é›†ç¯„ä¾‹
                if len(separation_stats['samples']) < 3:
                    content_preview = all_docs['documents'][i][:80].replace('\n', ' ')
                    separation_stats['samples'].append({
                        'source_file': metadata.get('source_file'),
                        'code_block_count': code_block_count,
                        'content_preview': content_preview
                    })

        if separation_stats['documents_with_code_blocks'] > 0:
            separation_stats['avg_code_blocks_per_doc'] = (
                separation_stats['total_code_blocks'] /
                separation_stats['documents_with_code_blocks']
            )

        print(f"åŒ…å«ç¨‹å¼ç¢¼å¡Šçš„æ–‡æª”: {separation_stats['documents_with_code_blocks']} / {separation_stats['total_documents']}")
        print(f"ç¸½ç¨‹å¼ç¢¼å¡Šæ•¸: {separation_stats['total_code_blocks']}")
        print(f"å¹³å‡æ¯æ–‡æª”ç¨‹å¼ç¢¼å¡Šæ•¸: {separation_stats['avg_code_blocks_per_doc']:.2f}")
        print(f"å–®ä¸€æ–‡æª”æœ€å¤šç¨‹å¼ç¢¼å¡Šæ•¸: {separation_stats['max_code_blocks_in_doc']}")

        print(f"\nç¨‹å¼ç¢¼åˆ†é›¢ç¯„ä¾‹:")
        for sample in separation_stats['samples']:
            print(f"\n  ğŸ“„ {sample['source_file']}")
            print(f"     ä»£ç¢¼å¡Šæ•¸: {sample['code_block_count']}")
            print(f"     æ–‡æœ¬é è¦½: {sample['content_preview']}...")

        return separation_stats

    def _verify_search(self) -> Dict[str, Any]:
        """é©—è­‰æœå°‹åŠŸèƒ½"""
        print(f"\n[4] æœå°‹åŠŸèƒ½é©—è­‰")
        print("-" * 80)

        test_queries = [
            ("ç¨‹å¼ç¢¼å¯©æŸ¥æ¨™æº–", 3),
            ("æ¸¬è©¦ç·¨å¯«", 3),
            ("Spring Boot é…ç½®", 2),
        ]

        search_stats = {
            'total_queries': len(test_queries),
            'successful_queries': 0,
            'results_with_code_blocks': 0,
            'query_results': []
        }

        for query, top_k in test_queries:
            try:
                results = self.vector_store.search_knowledge(query, top_k=top_k)

                if results:
                    search_stats['successful_queries'] += 1

                    query_result = {
                        'query': query,
                        'result_count': len(results),
                        'results': []
                    }

                    for i, result in enumerate(results[:top_k], 1):
                        result_info = {
                            'rank': i,
                            'topic': result.get('topic', 'N/A'),
                            'similarity': result.get('similarity', 0),
                            'has_code_blocks': 'code_blocks' in result and len(result['code_blocks']) > 0,
                            'code_block_count': len(result.get('code_blocks', []))
                        }
                        query_result['results'].append(result_info)

                        if result_info['has_code_blocks']:
                            search_stats['results_with_code_blocks'] += 1

                    search_stats['query_results'].append(query_result)

            except Exception as e:
                print(f"  âŒ æŸ¥è©¢å¤±æ•—: {query} - {str(e)}")

        print(f"æˆåŠŸçš„æŸ¥è©¢: {search_stats['successful_queries']} / {search_stats['total_queries']}")
        print(f"åŒ…å«ç¨‹å¼ç¢¼å¡Šçš„çµæœ: {search_stats['results_with_code_blocks']}")

        print(f"\næŸ¥è©¢çµæœè©³æƒ…:")
        for qr in search_stats['query_results']:
            print(f"\n  ğŸ” æŸ¥è©¢: {qr['query']}")
            print(f"     çµæœæ•¸: {qr['result_count']}")
            for res in qr['results']:
                similarity_indicator = "â­" * min(5, int(res['similarity'] * 5))
                print(f"     {res['rank']}. {res['topic']} {similarity_indicator}")
                if res['has_code_blocks']:
                    print(f"        [åŒ…å« {res['code_block_count']} å€‹ä»£ç¢¼å¡Š]")

        return search_stats

    def _analyze_performance(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æŒ‡æ¨™"""
        print(f"\n[5] æ€§èƒ½åˆ†æ")
        print("-" * 80)

        collection = self.vector_store.collection
        all_docs = collection.get()

        # è¨ˆç®—æ–‡æœ¬å¤§å°çµ±è¨ˆ
        text_sizes = [len(doc) for doc in all_docs['documents']]
        code_blocks = []

        for metadata in all_docs['metadatas']:
            if 'code_blocks' in metadata:
                try:
                    blocks = json.loads(metadata['code_blocks'])
                    code_blocks.extend(blocks)
                except:
                    pass

        # å–å¾— embedding ç¶­åº¦ï¼ˆä¸åŒæ–¼ embeddings åˆ—è¡¨ï¼‰
        embedding_dimension = 384  # paraphrase-multilingual-MiniLM-L12-v2 çš„ç¶­åº¦

        performance = {
            'total_documents': len(all_docs['ids']),
            'embedding_dimension': embedding_dimension,
            'avg_text_size': sum(text_sizes) / len(text_sizes) if text_sizes else 0,
            'min_text_size': min(text_sizes) if text_sizes else 0,
            'max_text_size': max(text_sizes) if text_sizes else 0,
            'total_code_blocks': len(code_blocks),
        }

        print(f"Embedding ç¶­åº¦: {performance['embedding_dimension']}")
        print(f"å¹³å‡æ–‡æœ¬å¤§å°: {performance['avg_text_size']:.0f} å­—ç¬¦")
        print(f"æ–‡æœ¬å¤§å°ç¯„åœ: {performance['min_text_size']} - {performance['max_text_size']} å­—ç¬¦")
        print(f"ç¸½ç¨‹å¼ç¢¼å¡Šæ•¸: {performance['total_code_blocks']}")

        # ä¼°ç®— embedding å¤§å°ç¯€çœ
        if code_blocks:
            total_code_size = sum(len(block.get('code', '')) for block in code_blocks)
            total_embedding_tokens = sum(text_sizes) + total_code_size
            text_only_tokens = sum(text_sizes)
            savings_pct = (1 - text_only_tokens / total_embedding_tokens) * 100 if total_embedding_tokens > 0 else 0

            print(f"\nç¨‹å¼ç¢¼åˆ†é›¢ç¯€çœï¼š")
            print(f"  åŒ…å«ç¨‹å¼ç¢¼çš„ tokens: ~{total_embedding_tokens:,}")
            print(f"  å¯¦éš› embedding çš„ tokens: ~{text_only_tokens:,}")
            print(f"  ç¯€çœ: {savings_pct:.1f}%")

        return performance

    def _generate_recommendations(self, results: Dict) -> List[str]:
        """æ ¹æ“šé©—è­‰çµæœç”Ÿæˆå»ºè­°"""
        recommendations = []

        # æª¢æŸ¥æ–‡æª”æ•¸é‡
        collection_stats = results['sections'].get('collection_stats', {})
        if collection_stats.get('total_documents', 0) == 0:
            recommendations.append("âš ï¸  æ²’æœ‰æ–‡æª”è¢«ç´¢å¼•ï¼Œè«‹æª¢æŸ¥ ingest è…³æœ¬")

        # æª¢æŸ¥å…ƒæ•¸æ“š
        metadata = results['sections'].get('metadata_validation', {})
        missing_fields = metadata.get('missing_fields', {})
        for field, count in missing_fields.items():
            if count > 0:
                recommendations.append(f"âš ï¸  {count} å€‹æ–‡æª”ç¼ºå°‘ '{field}' æ¬„ä½")

        # æª¢æŸ¥ä»£ç¢¼åˆ†é›¢
        separation = results['sections'].get('code_separation', {})
        code_coverage = (
            separation.get('documents_with_code_blocks', 0) /
            separation.get('total_documents', 1) * 100
            if separation.get('total_documents', 0) > 0 else 0
        )
        if code_coverage < 10:
            recommendations.append(f"â„¹ï¸  åªæœ‰ {code_coverage:.1f}% çš„æ–‡æª”åŒ…å«ç¨‹å¼ç¢¼å¡Šï¼Œé€™æ˜¯æ­£å¸¸çš„å¦‚æœå¤§å¤šæ•¸æ–‡æª”æ˜¯ç´”æ–‡æœ¬")

        # æª¢æŸ¥æœå°‹
        search = results['sections'].get('search_validation', {})
        if search.get('successful_queries', 0) < search.get('total_queries', 1):
            recommendations.append("âš ï¸  æŸäº›æœå°‹æŸ¥è©¢å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤æ—¥èªŒ")

        if not recommendations:
            recommendations.append("âœ… æ‰€æœ‰é©—è­‰é€šéï¼ç³»çµ±ç‹€æ…‹è‰¯å¥½ã€‚")

        return recommendations

    def _get_timestamp(self) -> str:
        """å–å¾—æ™‚é–“æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()

    def print_summary(self, results: Dict) -> None:
        """åˆ—å°æ‘˜è¦"""
        print(f"\n{self.separator_line}")
        print("é©—è­‰çµæœæ‘˜è¦".center(80))
        print(self.separator_line)

        print(f"\nå»ºè­°:")
        for rec in results['recommendations']:
            print(f"  {rec}")

        print(f"\n{self.separator_line}\n")


def main():
    """ä¸»å‡½æ•¸"""
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    chroma_db_dir = project_root / 'chroma_db'

    print(f"ChromaDB ç›®éŒ„: {chroma_db_dir}")

    # åˆå§‹åŒ–å‘é‡å­˜å„²
    print("\nåˆå§‹åŒ–å‘é‡å­˜å„²...")
    vector_store = VectorStoreService(
        db_path=str(chroma_db_dir),
        collection_name="ai_documentation"
    )

    # é‹è¡Œé©—è­‰
    verifier = AIDocsV2Verifier(vector_store)
    results = verifier.run_verification()
    verifier.print_summary(results)


if __name__ == '__main__':
    main()
