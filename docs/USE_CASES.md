# PostgreSQL MCP Server - å¸¸è¦‹ä½¿ç”¨å ´æ™¯ ğŸ¯

æœ¬æ–‡ä»¶å±•ç¤º PostgreSQL MCP Server åœ¨å¯¦éš›å·¥ä½œä¸­çš„å…¸å‹æ‡‰ç”¨å ´æ™¯å’Œæœ€ä½³å¯¦å‹™ã€‚

## ğŸ“‹ å ´æ™¯åˆ†é¡

- [è³‡æ–™é·ç§»èˆ‡è½‰æ›](#-è³‡æ–™é·ç§»èˆ‡è½‰æ›)
- [è³‡æ–™åˆ†æèˆ‡å ±è¡¨](#-è³‡æ–™åˆ†æèˆ‡å ±è¡¨)
- [è³‡æ–™åº«ç®¡ç†èˆ‡ç¶­é‹](#-è³‡æ–™åº«ç®¡ç†èˆ‡ç¶­é‹)
- [é–‹ç™¼èˆ‡æ¸¬è©¦æ”¯æ´](#-é–‹ç™¼èˆ‡æ¸¬è©¦æ”¯æ´)
- [ç›£æ§èˆ‡è­¦å ±](#-ç›£æ§èˆ‡è­¦å ±)
- [å•†æ¥­æ™ºæ…§æ‡‰ç”¨](#-å•†æ¥­æ™ºæ…§æ‡‰ç”¨)

## ğŸ”„ è³‡æ–™é·ç§»èˆ‡è½‰æ›

### å ´æ™¯ 1.1: ç³»çµ±å‡ç´šè³‡æ–™é·ç§»

**èƒŒæ™¯**: å…¬å¸è¦å¾èˆŠçš„ ERP ç³»çµ±é·ç§»åˆ°æ–°ç³»çµ±ï¼Œéœ€è¦å°‡ 10 è¬ç­†å®¢æˆ¶è³‡æ–™å’Œ 50 è¬ç­†äº¤æ˜“ç´€éŒ„é·ç§»éå»ã€‚

**æŒ‘æˆ°**:
- è³‡æ–™çµæ§‹ä¸åŒ
- éœ€è¦è³‡æ–™æ¸…ç†å’Œè½‰æ›
- ç¢ºä¿è³‡æ–™å®Œæ•´æ€§
- æœ€å°åŒ–åœæ©Ÿæ™‚é–“

**è§£æ±ºæ–¹æ¡ˆ**:

```python
# ä½¿ç”¨ MCP å·¥å…·é€²è¡Œæ™ºèƒ½è³‡æ–™é·ç§»
async def migrate_erp_data():
    # 1. åˆ†æä¾†æºå’Œç›®æ¨™è¡¨çµæ§‹
    old_customers = await get_table_schema("old_erp", "customers")
    new_customers = await get_table_schema("new_erp", "customer_profiles")

    # 2. æ‰¹æ¬¡æå–å’Œè½‰æ›è³‡æ–™
    batch_size = 1000
    offset = 0

    while True:
        # åˆ†æ‰¹æå–è³‡æ–™
        source_data = await execute_query(
            "old_erp",
            """SELECT customer_id, company_name, contact_person,
                      phone, email, address, registration_date
               FROM customers
               ORDER BY customer_id
               LIMIT $1 OFFSET $2""",
            [batch_size, offset]
        )

        if not source_data["rows"]:
            break

        # è³‡æ–™è½‰æ›å’Œæ¸…ç†
        transformed_data = []
        for customer in source_data["rows"]:
            transformed_data.append({
                "query": """
                    INSERT INTO customer_profiles (
                        legacy_id, business_name, primary_contact,
                        contact_phone, email_address, business_address,
                        onboarding_date, data_source
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, 'ERP_MIGRATION')
                """,
                "params": [
                    customer["customer_id"],
                    customer["company_name"].strip().title(),
                    customer["contact_person"],
                    clean_phone(customer["phone"]),
                    customer["email"].lower(),
                    customer["address"],
                    customer["registration_date"]
                ]
            })

        # äº‹å‹™æ€§æ‰¹æ¬¡æ’å…¥
        result = await execute_transaction("new_erp", transformed_data)
        print(f"âœ… å·²é·ç§» {len(transformed_data)} ç­†å®¢æˆ¶è³‡æ–™")

        offset += batch_size
```

**å„ªå‹¢**:
- âœ… è‡ªå‹•åŒ–è³‡æ–™è½‰æ›
- âœ… äº‹å‹™ä¿è­‰è³‡æ–™ä¸€è‡´æ€§
- âœ… æ‰¹æ¬¡è™•ç†æå‡æ•ˆèƒ½
- âœ… LLM æ™ºèƒ½è™•ç†è³‡æ–™å“è³ªå•é¡Œ

### å ´æ™¯ 1.2: å¤šè³‡æ–™æºè³‡æ–™æ•´åˆ

**èƒŒæ™¯**: å°‡ä¾†è‡ª CRMã€ERPã€é›»å•†å¹³å°çš„å®¢æˆ¶è³‡æ–™æ•´åˆåˆ°çµ±ä¸€çš„å®¢æˆ¶è³‡æ–™å¹³å°ã€‚

```python
async def integrate_customer_data():
    # é€£æ¥å¤šå€‹è³‡æ–™æº
    data_sources = {
        "crm": {"host": "crm-db.company.com", "database": "salesforce"},
        "erp": {"host": "erp-db.company.com", "database": "sap_data"},
        "ecommerce": {"host": "shop-db.company.com", "database": "magento"}
    }

    for source_id, config in data_sources.items():
        await add_connection(source_id, **config)

    # çµ±ä¸€å®¢æˆ¶è³‡æ–™æ ¼å¼
    unified_customers = []

    # å¾ CRM æå–æ½›åœ¨å®¢æˆ¶
    crm_leads = await execute_query("crm", """
        SELECT id, first_name, last_name, email, phone, company,
               created_date, lead_source, status
        FROM leads WHERE status IN ('qualified', 'converted')
    """)

    # å¾ ERP æå–ç¾æœ‰å®¢æˆ¶
    erp_customers = await execute_query("erp", """
        SELECT customer_code, company_name, contact_email,
               contact_phone, industry, annual_revenue
        FROM customers WHERE active = true
    """)

    # å¾é›»å•†å¹³å°æå–ç·šä¸Šå®¢æˆ¶
    ecommerce_users = await execute_query("ecommerce", """
        SELECT user_id, email, first_name, last_name,
               total_orders, total_spent, last_order_date
        FROM customer_summary WHERE total_orders > 0
    """)

    # æ™ºèƒ½è³‡æ–™å»é‡å’Œæ•´åˆï¼ˆç”± LLM è™•ç†è¤‡é›œé‚è¼¯ï¼‰
    # LLM å¯ä»¥åŸºæ–¼ emailã€phoneã€company name é€²è¡Œæ™ºèƒ½åŒ¹é…
```

## ğŸ“Š è³‡æ–™åˆ†æèˆ‡å ±è¡¨

### å ´æ™¯ 2.1: å³æ™‚æ¥­å‹™å„€è¡¨æ¿

**èƒŒæ™¯**: ç®¡ç†åœ˜éšŠéœ€è¦å³æ™‚ç›£æ§éŠ·å”®ç¸¾æ•ˆã€å®¢æˆ¶æ´»å‹•å’Œåº«å­˜ç‹€æ³ã€‚

```python
async def generate_business_dashboard():
    # éŠ·å”®ç¸¾æ•ˆæŒ‡æ¨™
    sales_metrics = await execute_query("analytics", """
        WITH daily_sales AS (
            SELECT DATE(order_date) as sale_date,
                   COUNT(*) as orders_count,
                   SUM(total_amount) as revenue,
                   COUNT(DISTINCT customer_id) as unique_customers
            FROM orders
            WHERE order_date >= CURRENT_DATE - INTERVAL '30 days'
            GROUP BY DATE(order_date)
        )
        SELECT
            sale_date,
            orders_count,
            revenue,
            unique_customers,
            revenue / NULLIF(orders_count, 0) as avg_order_value,
            LAG(revenue) OVER (ORDER BY sale_date) as prev_day_revenue
        FROM daily_sales
        ORDER BY sale_date DESC
        LIMIT 30
    """)

    # å®¢æˆ¶åˆ†æ
    customer_insights = await execute_query("analytics", """
        SELECT
            customer_segment,
            COUNT(*) as customer_count,
            AVG(lifetime_value) as avg_ltv,
            SUM(CASE WHEN last_order_date > CURRENT_DATE - INTERVAL '30 days'
                THEN 1 ELSE 0 END) as active_customers,
            AVG(satisfaction_score) as avg_satisfaction
        FROM customer_profiles
        GROUP BY customer_segment
        ORDER BY avg_ltv DESC
    """)

    # åº«å­˜è­¦å ±
    inventory_alerts = await execute_query("warehouse", """
        SELECT
            product_code,
            product_name,
            current_stock,
            reorder_level,
            supplier_name,
            lead_time_days,
            CASE
                WHEN current_stock <= 0 THEN 'OUT_OF_STOCK'
                WHEN current_stock <= reorder_level THEN 'LOW_STOCK'
                ELSE 'NORMAL'
            END as stock_status
        FROM inventory_view
        WHERE current_stock <= reorder_level * 1.2
        ORDER BY
            CASE stock_status
                WHEN 'OUT_OF_STOCK' THEN 1
                WHEN 'LOW_STOCK' THEN 2
                ELSE 3
            END,
            current_stock ASC
    """)

    return {
        "sales_performance": sales_metrics["rows"],
        "customer_insights": customer_insights["rows"],
        "inventory_alerts": inventory_alerts["rows"],
        "generated_at": datetime.utcnow().isoformat()
    }
```

### å ´æ™¯ 2.2: æ·±åº¦è³‡æ–™æ¢ç´¢åˆ†æ

**èƒŒæ™¯**: è¡ŒéŠ·åœ˜éšŠæƒ³äº†è§£ä¸åŒå®¢æˆ¶ç¾¤é«”çš„è³¼è²·è¡Œç‚ºæ¨¡å¼ï¼Œä»¥å„ªåŒ–è¡ŒéŠ·ç­–ç•¥ã€‚

```python
async def customer_behavior_analysis():
    # RFM åˆ†æ (Recency, Frequency, Monetary)
    rfm_analysis = await execute_query("marketing", """
        WITH customer_rfm AS (
            SELECT
                customer_id,
                EXTRACT(DAYS FROM CURRENT_DATE - MAX(order_date)) as recency_days,
                COUNT(DISTINCT order_id) as frequency,
                SUM(total_amount) as monetary_value
            FROM orders
            WHERE order_date >= CURRENT_DATE - INTERVAL '365 days'
            GROUP BY customer_id
        ),
        rfm_scores AS (
            SELECT *,
                NTILE(5) OVER (ORDER BY recency_days DESC) as recency_score,
                NTILE(5) OVER (ORDER BY frequency) as frequency_score,
                NTILE(5) OVER (ORDER BY monetary_value) as monetary_score
            FROM customer_rfm
        )
        SELECT
            recency_score,
            frequency_score,
            monetary_score,
            COUNT(*) as customer_count,
            AVG(monetary_value) as avg_value,
            CASE
                WHEN recency_score >= 4 AND frequency_score >= 4 THEN 'Champions'
                WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'
                WHEN recency_score >= 4 AND frequency_score <= 2 THEN 'New Customers'
                WHEN recency_score <= 2 AND frequency_score >= 3 THEN 'At Risk'
                WHEN recency_score <= 2 AND frequency_score <= 2 THEN 'Lost Customers'
                ELSE 'Regular Customers'
            END as customer_segment
        FROM rfm_scores
        GROUP BY recency_score, frequency_score, monetary_score
        ORDER BY recency_score DESC, frequency_score DESC, monetary_score DESC
    """)

    # ç”¢å“é—œè¯åˆ†æ
    product_affinity = await execute_query("marketing", """
        WITH product_pairs AS (
            SELECT
                oi1.product_id as product_a,
                oi2.product_id as product_b,
                COUNT(DISTINCT oi1.order_id) as cooccurrence_count
            FROM order_items oi1
            JOIN order_items oi2 ON oi1.order_id = oi2.order_id
                AND oi1.product_id < oi2.product_id
            JOIN orders o ON oi1.order_id = o.order_id
            WHERE o.order_date >= CURRENT_DATE - INTERVAL '180 days'
            GROUP BY oi1.product_id, oi2.product_id
            HAVING COUNT(DISTINCT oi1.order_id) >= 10
        )
        SELECT
            pa.name as product_a_name,
            pb.name as product_b_name,
            pp.cooccurrence_count,
            ROUND(
                pp.cooccurrence_count * 100.0 /
                (SELECT COUNT(DISTINCT order_id) FROM order_items WHERE product_id = pp.product_a),
                2
            ) as lift_percentage
        FROM product_pairs pp
        JOIN products pa ON pp.product_a = pa.product_id
        JOIN products pb ON pp.product_b = pb.product_id
        ORDER BY pp.cooccurrence_count DESC
        LIMIT 20
    """)
```

## ğŸ› ï¸ è³‡æ–™åº«ç®¡ç†èˆ‡ç¶­é‹

### å ´æ™¯ 3.1: è‡ªå‹•åŒ–è³‡æ–™åº«å¥åº·æª¢æŸ¥

**èƒŒæ™¯**: DBA åœ˜éšŠéœ€è¦æ¯æ—¥è‡ªå‹•æª¢æŸ¥å¤šå€‹ç”Ÿç”¢è³‡æ–™åº«çš„å¥åº·ç‹€æ³ã€‚

```python
async def database_health_monitoring():
    databases = [
        {"id": "prod_app", "host": "prod-db-01.company.com"},
        {"id": "prod_analytics", "host": "analytics-db.company.com"},
        {"id": "prod_warehouse", "host": "dw-db.company.com"}
    ]

    health_report = {"timestamp": datetime.utcnow(), "databases": []}

    for db_config in databases:
        db_id = db_config["id"]

        try:
            # é€£ç·šæ¸¬è©¦
            conn_test = await test_connection(db_id)

            # ç³»çµ±æŒ‡æ¨™æª¢æŸ¥
            system_metrics = await execute_query(db_id, """
                SELECT
                    current_database(),
                    pg_database_size(current_database()) / 1024 / 1024 / 1024.0 as db_size_gb,
                    (SELECT count(*) FROM pg_stat_activity WHERE state = 'active') as active_connections,
                    (SELECT count(*) FROM pg_stat_activity) as total_connections,
                    (SELECT setting::int FROM pg_settings WHERE name = 'max_connections') as max_connections
            """)

            # é–ç­‰å¾…æª¢æŸ¥
            lock_waits = await execute_query(db_id, """
                SELECT
                    blocked_locks.pid AS blocked_pid,
                    blocked_activity.usename AS blocked_user,
                    blocking_locks.pid AS blocking_pid,
                    blocking_activity.usename AS blocking_user,
                    blocked_activity.query AS blocked_statement,
                    blocking_activity.query AS current_statement_in_blocking_process
                FROM pg_catalog.pg_locks blocked_locks
                JOIN pg_catalog.pg_stat_activity blocked_activity
                    ON blocked_activity.pid = blocked_locks.pid
                JOIN pg_catalog.pg_locks blocking_locks
                    ON blocking_locks.locktype = blocked_locks.locktype
                    AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database
                    AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
                    AND blocking_locks.pid != blocked_locks.pid
                JOIN pg_catalog.pg_stat_activity blocking_activity
                    ON blocking_activity.pid = blocking_locks.pid
                WHERE NOT blocked_locks.granted
            """)

            # æ…¢æŸ¥è©¢æª¢æŸ¥
            slow_queries = await execute_query(db_id, """
                SELECT
                    query,
                    calls,
                    total_time,
                    mean_time,
                    stddev_time,
                    rows
                FROM pg_stat_statements
                WHERE mean_time > 1000
                ORDER BY mean_time DESC
                LIMIT 10
            """)

            db_health = {
                "database": db_id,
                "status": "healthy" if conn_test["status"] == "connected" else "unhealthy",
                "metrics": system_metrics["rows"][0],
                "lock_waits": len(lock_waits["rows"]),
                "slow_queries_count": len(slow_queries["rows"]),
                "issues": []
            }

            # å¥åº·æª¢æŸ¥é‚è¼¯
            metrics = system_metrics["rows"][0]
            if metrics["active_connections"] > metrics["max_connections"] * 0.8:
                db_health["issues"].append("High connection usage")

            if metrics["db_size_gb"] > 100:  # è¶…é 100GB
                db_health["issues"].append("Large database size")

            if len(lock_waits["rows"]) > 0:
                db_health["issues"].append(f"{len(lock_waits['rows'])} lock waits detected")

            health_report["databases"].append(db_health)

        except Exception as e:
            health_report["databases"].append({
                "database": db_id,
                "status": "error",
                "error": str(e)
            })

    return health_report
```

### å ´æ™¯ 3.2: è‡ªå‹•åŒ–è³‡æ–™åº«ç¶­è­·

**èƒŒæ™¯**: å®šæœŸåŸ·è¡Œè³‡æ–™åº«ç¶­è­·ä»»å‹™ï¼Œå¦‚çµ±è¨ˆè³‡è¨Šæ›´æ–°ã€ç´¢å¼•é‡å»ºã€è³‡æ–™æ¸…ç†ã€‚

```python
async def automated_maintenance():
    maintenance_tasks = []

    # æ›´æ–°çµ±è¨ˆè³‡è¨Š
    stats_update = await execute_query("maintenance_db", """
        SELECT schemaname, tablename, n_dead_tup, n_live_tup,
               CASE WHEN n_live_tup > 0
                    THEN n_dead_tup::float / n_live_tup::float
                    ELSE 0 END as dead_ratio
        FROM pg_stat_user_tables
        WHERE n_dead_tup > 1000 AND
              (n_dead_tup::float / GREATEST(n_live_tup, 1)::float) > 0.1
    """)

    # å°éœ€è¦çš„è¡¨åŸ·è¡Œ VACUUM ANALYZE
    for table_info in stats_update["rows"]:
        schema = table_info["schemaname"]
        table = table_info["tablename"]

        await execute_query("maintenance_db",
            f"VACUUM ANALYZE {schema}.{table}")

        maintenance_tasks.append(f"Vacuumed {schema}.{table}")

    # æª¢æŸ¥ä¸¦é‡å»ºç¢ç‰‡åŒ–ç´¢å¼•
    fragmented_indexes = await execute_query("maintenance_db", """
        SELECT
            schemaname,
            tablename,
            indexname,
            pg_relation_size(indexrelid) as index_size,
            pg_stat_get_blocks_fetched(indexrelid) -
            pg_stat_get_blocks_hit(indexrelid) as blocks_read
        FROM pg_stat_user_indexes
        WHERE pg_relation_size(indexrelid) > 100 * 1024 * 1024  -- 100MB+
        ORDER BY blocks_read DESC
        LIMIT 10
    """)

    # æ¸…ç†éæœŸè³‡æ–™
    cleanup_result = await execute_transaction("maintenance_db", [
        {
            "query": "DELETE FROM audit_logs WHERE created_at < $1",
            "params": [datetime.utcnow() - timedelta(days=90)]
        },
        {
            "query": "DELETE FROM session_tokens WHERE expires_at < $1",
            "params": [datetime.utcnow()]
        }
    ])

    maintenance_tasks.append(f"Cleaned up old data: {cleanup_result['affected_rows']} rows")

    return maintenance_tasks
```

## ğŸš€ é–‹ç™¼èˆ‡æ¸¬è©¦æ”¯æ´

### å ´æ™¯ 4.1: æ¸¬è©¦è³‡æ–™ç”Ÿæˆ

**èƒŒæ™¯**: é–‹ç™¼åœ˜éšŠéœ€è¦ç”Ÿæˆæ“¬çœŸçš„æ¸¬è©¦è³‡æ–™ä¾†é€²è¡ŒåŠŸèƒ½æ¸¬è©¦å’Œæ•ˆèƒ½æ¸¬è©¦ã€‚

```python
async def generate_test_data():
    # ç”Ÿæˆæ¸¬è©¦å®¢æˆ¶
    test_customers = []
    for i in range(1000):
        test_customers.append({
            "query": """
                INSERT INTO customers (name, email, phone, company, industry, created_at)
                VALUES ($1, $2, $3, $4, $5, $6)
            """,
            "params": [
                f"Test Customer {i+1}",
                f"customer{i+1}@testdomain.com",
                f"+1-555-{1000+i:04d}",
                f"Test Company {i+1}",
                random.choice(["Technology", "Healthcare", "Finance", "Retail", "Manufacturing"]),
                datetime.utcnow() - timedelta(days=random.randint(1, 365))
            ]
        })

    # æ‰¹æ¬¡æ’å…¥å®¢æˆ¶è³‡æ–™
    await execute_transaction("test_db", test_customers)

    # ç”Ÿæˆæ¸¬è©¦è¨‚å–®
    customers = await execute_query("test_db", "SELECT customer_id FROM customers")

    test_orders = []
    for _ in range(5000):
        customer_id = random.choice(customers["rows"])["customer_id"]
        order_date = datetime.utcnow() - timedelta(days=random.randint(1, 180))

        test_orders.append({
            "query": """
                INSERT INTO orders (customer_id, order_date, total_amount, status)
                VALUES ($1, $2, $3, $4)
            """,
            "params": [
                customer_id,
                order_date,
                round(random.uniform(50, 5000), 2),
                random.choice(["pending", "processing", "shipped", "delivered", "cancelled"])
            ]
        })

    await execute_transaction("test_db", test_orders)

    print("âœ… æ¸¬è©¦è³‡æ–™ç”Ÿæˆå®Œæˆï¼š1000 å€‹å®¢æˆ¶ï¼Œ5000 ç­†è¨‚å–®")
```

### å ´æ™¯ 4.2: è³‡æ–™åº« Schema æ¯”è¼ƒ

**èƒŒæ™¯**: ç¢ºä¿é–‹ç™¼ã€æ¸¬è©¦å’Œç”Ÿç”¢ç’°å¢ƒçš„è³‡æ–™åº«çµæ§‹ä¸€è‡´æ€§ã€‚

```python
async def compare_database_schemas():
    environments = ["dev_db", "test_db", "prod_db"]
    schema_comparison = {}

    for env in environments:
        # ç²å–æ‰€æœ‰è¡¨çš„çµæ§‹
        tables_info = await execute_query(env, """
            SELECT
                table_name,
                column_name,
                data_type,
                is_nullable,
                column_default,
                ordinal_position
            FROM information_schema.columns
            WHERE table_schema = 'public'
            ORDER BY table_name, ordinal_position
        """)

        # ç²å–ç´¢å¼•è³‡è¨Š
        indexes_info = await execute_query(env, """
            SELECT
                tablename,
                indexname,
                indexdef
            FROM pg_indexes
            WHERE schemaname = 'public'
            ORDER BY tablename, indexname
        """)

        schema_comparison[env] = {
            "tables": tables_info["rows"],
            "indexes": indexes_info["rows"]
        }

    # æ¯”è¼ƒå·®ç•°ï¼ˆé€™è£¡ LLM å¯ä»¥æ™ºèƒ½åˆ†æå·®ç•°ï¼‰
    differences = []

    # æ¯”è¼ƒè¡¨çµæ§‹
    dev_tables = {(t["table_name"], t["column_name"]): t for t in schema_comparison["dev_db"]["tables"]}
    prod_tables = {(t["table_name"], t["column_name"]): t for t in schema_comparison["prod_db"]["tables"]}

    # æ‰¾å‡ºç¼ºå¤±çš„æ¬„ä½
    missing_in_prod = set(dev_tables.keys()) - set(prod_tables.keys())
    missing_in_dev = set(prod_tables.keys()) - set(dev_tables.keys())

    if missing_in_prod:
        differences.append(f"Production missing columns: {missing_in_prod}")
    if missing_in_dev:
        differences.append(f"Development has extra columns: {missing_in_dev}")

    return {
        "schemas": schema_comparison,
        "differences": differences,
        "sync_required": len(differences) > 0
    }
```

## ğŸ“ˆ ç›£æ§èˆ‡è­¦å ±

### å ´æ™¯ 5.1: å³æ™‚æ•ˆèƒ½ç›£æ§

**èƒŒæ™¯**: ç›£æ§è³‡æ–™åº«æ•ˆèƒ½æŒ‡æ¨™ï¼Œåœ¨å‡ºç¾ç•°å¸¸æ™‚åŠæ™‚è­¦å ±ã€‚

```python
async def performance_monitoring():
    # æ”¶é›†æ•ˆèƒ½æŒ‡æ¨™
    performance_metrics = await execute_query("monitor_db", """
        SELECT
            -- é€£ç·šè³‡è¨Š
            (SELECT count(*) FROM pg_stat_activity WHERE state = 'active') as active_connections,
            (SELECT count(*) FROM pg_stat_activity WHERE state = 'idle in transaction') as idle_in_transaction,

            -- æŸ¥è©¢æ•ˆèƒ½
            (SELECT avg(query_start - xact_start) FROM pg_stat_activity
             WHERE state = 'active' AND query_start IS NOT NULL) as avg_query_duration,

            -- é–è³‡è¨Š
            (SELECT count(*) FROM pg_locks WHERE NOT granted) as waiting_locks,

            -- I/O çµ±è¨ˆ
            (SELECT sum(blks_read) FROM pg_stat_database) as total_blocks_read,
            (SELECT sum(blks_hit) FROM pg_stat_database) as total_blocks_hit,

            -- è³‡æ–™åº«å¤§å°
            pg_database_size(current_database()) / 1024 / 1024 / 1024.0 as db_size_gb
    """)

    metrics = performance_metrics["rows"][0]

    # è­¦å ±é–¾å€¼æª¢æŸ¥
    alerts = []

    if metrics["active_connections"] > 80:
        alerts.append({
            "level": "warning",
            "message": f"High active connections: {metrics['active_connections']}"
        })

    if metrics["idle_in_transaction"] > 10:
        alerts.append({
            "level": "critical",
            "message": f"Many idle in transaction: {metrics['idle_in_transaction']}"
        })

    if metrics["waiting_locks"] > 0:
        alerts.append({
            "level": "warning",
            "message": f"Waiting locks detected: {metrics['waiting_locks']}"
        })

    # è¨ˆç®—å¿«å–å‘½ä¸­ç‡
    cache_hit_ratio = (metrics["total_blocks_hit"] /
                      (metrics["total_blocks_hit"] + metrics["total_blocks_read"]) * 100)

    if cache_hit_ratio < 95:
        alerts.append({
            "level": "warning",
            "message": f"Low cache hit ratio: {cache_hit_ratio:.2f}%"
        })

    return {
        "timestamp": datetime.utcnow(),
        "metrics": metrics,
        "cache_hit_ratio": cache_hit_ratio,
        "alerts": alerts
    }
```

### å ´æ™¯ 5.2: è³‡æ–™å“è³ªç›£æ§

**èƒŒæ™¯**: æŒçºŒç›£æ§è³‡æ–™å“è³ªï¼Œç¢ºä¿è³‡æ–™çš„æº–ç¢ºæ€§å’Œå®Œæ•´æ€§ã€‚

```python
async def data_quality_monitoring():
    quality_checks = []

    # æª¢æŸ¥1: è³‡æ–™å®Œæ•´æ€§
    completeness_check = await execute_query("quality_db", """
        SELECT
            'customers' as table_name,
            'email' as column_name,
            COUNT(*) as total_rows,
            COUNT(email) as non_null_rows,
            COUNT(*) - COUNT(email) as null_rows,
            ROUND((COUNT(*) - COUNT(email)) * 100.0 / COUNT(*), 2) as null_percentage
        FROM customers

        UNION ALL

        SELECT
            'orders' as table_name,
            'customer_id' as column_name,
            COUNT(*) as total_rows,
            COUNT(customer_id) as non_null_rows,
            COUNT(*) - COUNT(customer_id) as null_rows,
            ROUND((COUNT(*) - COUNT(customer_id)) * 100.0 / COUNT(*), 2) as null_percentage
        FROM orders
    """)

    # æª¢æŸ¥2: è³‡æ–™ä¸€è‡´æ€§
    consistency_check = await execute_query("quality_db", """
        -- æª¢æŸ¥è¨‚å–®è¡¨ä¸­çš„å®¢æˆ¶IDæ˜¯å¦éƒ½å­˜åœ¨æ–¼å®¢æˆ¶è¡¨ä¸­
        SELECT
            'referential_integrity' as check_type,
            'orders.customer_id -> customers.customer_id' as check_description,
            COUNT(*) as violations
        FROM orders o
        LEFT JOIN customers c ON o.customer_id = c.customer_id
        WHERE c.customer_id IS NULL

        UNION ALL

        -- æª¢æŸ¥é›»å­éƒµä»¶æ ¼å¼
        SELECT
            'email_format' as check_type,
            'customers.email format validation' as check_description,
            COUNT(*) as violations
        FROM customers
        WHERE email NOT LIKE '%@%' OR email NOT LIKE '%.%'
    """)

    # æª¢æŸ¥3: è³‡æ–™æ–°é®®åº¦
    freshness_check = await execute_query("quality_db", """
        SELECT
            table_name,
            date_column,
            MAX(date_value) as latest_record,
            EXTRACT(HOURS FROM CURRENT_TIMESTAMP - MAX(date_value)) as hours_since_last_update
        FROM (
            SELECT 'orders' as table_name, 'order_date' as date_column, order_date as date_value FROM orders
            UNION ALL
            SELECT 'customers' as table_name, 'created_at' as date_column, created_at as date_value FROM customers
        ) data_dates
        GROUP BY table_name, date_column
    """)

    # ç”Ÿæˆå“è³ªå ±å‘Š
    quality_report = {
        "timestamp": datetime.utcnow(),
        "completeness": completeness_check["rows"],
        "consistency": consistency_check["rows"],
        "freshness": freshness_check["rows"],
        "overall_score": 100  # åŸºæ–¼æª¢æŸ¥çµæœè¨ˆç®—åˆ†æ•¸
    }

    # è¨ˆç®—æ•´é«”å“è³ªåˆ†æ•¸
    issues = 0
    for check in completeness_check["rows"]:
        if check["null_percentage"] > 5:
            issues += 1

    for check in consistency_check["rows"]:
        if check["violations"] > 0:
            issues += 2  # ä¸€è‡´æ€§å•é¡Œæ¬Šé‡è¼ƒé«˜

    quality_report["overall_score"] = max(0, 100 - (issues * 10))

    return quality_report
```

## ğŸ’¼ å•†æ¥­æ™ºæ…§æ‡‰ç”¨

### å ´æ™¯ 6.1: å®¢æˆ¶åƒ¹å€¼åˆ†æ

**èƒŒæ™¯**: è¡ŒéŠ·åœ˜éšŠéœ€è¦è­˜åˆ¥é«˜åƒ¹å€¼å®¢æˆ¶ä¸¦åˆ¶å®šé‡å°æ€§çš„è¡ŒéŠ·ç­–ç•¥ã€‚

```python
async def customer_value_analysis():
    # å®¢æˆ¶ç”Ÿå‘½é€±æœŸåƒ¹å€¼è¨ˆç®—
    clv_analysis = await execute_query("bi_db", """
        WITH customer_metrics AS (
            SELECT
                c.customer_id,
                c.registration_date,
                EXTRACT(DAYS FROM CURRENT_DATE - c.registration_date) as customer_age_days,
                COUNT(DISTINCT o.order_id) as total_orders,
                SUM(o.total_amount) as total_spent,
                AVG(o.total_amount) as avg_order_value,
                MAX(o.order_date) as last_order_date,
                EXTRACT(DAYS FROM CURRENT_DATE - MAX(o.order_date)) as days_since_last_order,
                MIN(o.order_date) as first_order_date,
                EXTRACT(DAYS FROM MAX(o.order_date) - MIN(o.order_date)) as customer_lifespan_days
            FROM customers c
            LEFT JOIN orders o ON c.customer_id = o.customer_id
            GROUP BY c.customer_id, c.registration_date
        ),
        clv_segments AS (
            SELECT *,
                CASE
                    WHEN total_spent >= 10000 AND days_since_last_order <= 30 THEN 'VIP_Active'
                    WHEN total_spent >= 5000 AND days_since_last_order <= 60 THEN 'High_Value'
                    WHEN total_spent >= 1000 AND days_since_last_order <= 90 THEN 'Medium_Value'
                    WHEN days_since_last_order > 180 THEN 'At_Risk'
                    WHEN total_orders = 0 THEN 'Never_Purchased'
                    ELSE 'Regular'
                END as value_segment,

                -- é æ¸¬æœªä¾†åƒ¹å€¼ï¼ˆç°¡åŒ–æ¨¡å‹ï¼‰
                CASE
                    WHEN customer_lifespan_days > 0 THEN
                        (total_spent / GREATEST(customer_lifespan_days, 1)) * 365 * 2
                    ELSE avg_order_value * 4
                END as predicted_annual_value

            FROM customer_metrics
        )
        SELECT
            value_segment,
            COUNT(*) as customer_count,
            AVG(total_spent) as avg_total_spent,
            AVG(total_orders) as avg_total_orders,
            AVG(avg_order_value) as avg_order_value,
            AVG(predicted_annual_value) as avg_predicted_annual_value,
            SUM(total_spent) as segment_total_revenue
        FROM clv_segments
        GROUP BY value_segment
        ORDER BY avg_predicted_annual_value DESC
    """)

    # ç”¢å“è¦ªå’Œæ€§åˆ†æ
    product_affinity = await execute_query("bi_db", """
        WITH product_cooccurrence AS (
            SELECT
                oi1.product_id as product_a,
                oi2.product_id as product_b,
                COUNT(DISTINCT oi1.order_id) as cooccurrence_count,
                COUNT(DISTINCT oi1.order_id) * 1.0 /
                    (SELECT COUNT(DISTINCT order_id) FROM order_items WHERE product_id = oi1.product_id) as confidence
            FROM order_items oi1
            JOIN order_items oi2 ON oi1.order_id = oi2.order_id AND oi1.product_id != oi2.product_id
            GROUP BY oi1.product_id, oi2.product_id
            HAVING COUNT(DISTINCT oi1.order_id) >= 5
        )
        SELECT
            pa.name as product_a_name,
            pb.name as product_b_name,
            pc.cooccurrence_count,
            ROUND(pc.confidence * 100, 2) as confidence_percentage
        FROM product_cooccurrence pc
        JOIN products pa ON pc.product_a = pa.product_id
        JOIN products pb ON pc.product_b = pb.product_id
        WHERE pc.confidence > 0.3
        ORDER BY pc.confidence DESC
        LIMIT 20
    """)

    return {
        "customer_segments": clv_analysis["rows"],
        "product_recommendations": product_affinity["rows"],
        "analysis_date": datetime.utcnow()
    }
```

### å ´æ™¯ 6.2: éŠ·å”®é æ¸¬åˆ†æ

**èƒŒæ™¯**: åŸºæ–¼æ­·å²è³‡æ–™é æ¸¬æœªä¾†éŠ·å”®è¶¨å‹¢ï¼Œå”åŠ©åº«å­˜è¦åŠƒå’Œæ¥­å‹™æ±ºç­–ã€‚

```python
async def sales_forecasting():
    # æ­·å²éŠ·å”®è¶¨å‹¢åˆ†æ
    historical_sales = await execute_query("forecast_db", """
        WITH monthly_sales AS (
            SELECT
                DATE_TRUNC('month', order_date) as month,
                COUNT(DISTINCT order_id) as orders_count,
                SUM(total_amount) as monthly_revenue,
                COUNT(DISTINCT customer_id) as unique_customers,
                AVG(total_amount) as avg_order_value
            FROM orders
            WHERE order_date >= CURRENT_DATE - INTERVAL '24 months'
            GROUP BY DATE_TRUNC('month', order_date)
        ),
        sales_with_growth AS (
            SELECT *,
                LAG(monthly_revenue, 1) OVER (ORDER BY month) as prev_month_revenue,
                LAG(monthly_revenue, 12) OVER (ORDER BY month) as same_month_last_year,
                (monthly_revenue - LAG(monthly_revenue, 1) OVER (ORDER BY month)) /
                    NULLIF(LAG(monthly_revenue, 1) OVER (ORDER BY month), 0) * 100 as mom_growth,
                (monthly_revenue - LAG(monthly_revenue, 12) OVER (ORDER BY month)) /
                    NULLIF(LAG(monthly_revenue, 12) OVER (ORDER BY month), 0) * 100 as yoy_growth
            FROM monthly_sales
        )
        SELECT
            month,
            monthly_revenue,
            orders_count,
            unique_customers,
            avg_order_value,
            ROUND(mom_growth, 2) as mom_growth_pct,
            ROUND(yoy_growth, 2) as yoy_growth_pct,
            -- ç°¡å–®çš„ç§»å‹•å¹³å‡é æ¸¬
            AVG(monthly_revenue) OVER (
                ORDER BY month ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
            ) as ma3_forecast
        FROM sales_with_growth
        ORDER BY month
    """)

    # å­£ç¯€æ€§åˆ†æ
    seasonal_analysis = await execute_query("forecast_db", """
        SELECT
            EXTRACT(MONTH FROM order_date) as month,
            EXTRACT(QUARTER FROM order_date) as quarter,
            COUNT(*) as orders_count,
            SUM(total_amount) as total_revenue,
            AVG(total_amount) as avg_order_value,
            -- è¨ˆç®—ç›¸å°æ–¼å¹´å¹³å‡çš„å­£ç¯€æ€§æŒ‡æ•¸
            SUM(total_amount) / (
                SELECT SUM(total_amount) / 12.0
                FROM orders
                WHERE EXTRACT(YEAR FROM order_date) = EXTRACT(YEAR FROM order_date)
            ) as seasonal_index
        FROM orders
        WHERE order_date >= CURRENT_DATE - INTERVAL '36 months'
        GROUP BY EXTRACT(MONTH FROM order_date), EXTRACT(QUARTER FROM order_date)
        ORDER BY month
    """)

    # ç”¢å“éŠ·å”®é æ¸¬
    product_forecast = await execute_query("forecast_db", """
        WITH product_monthly_sales AS (
            SELECT
                p.product_id,
                p.name as product_name,
                p.category,
                DATE_TRUNC('month', o.order_date) as month,
                SUM(oi.quantity) as units_sold,
                SUM(oi.quantity * oi.unit_price) as revenue
            FROM products p
            JOIN order_items oi ON p.product_id = oi.product_id
            JOIN orders o ON oi.order_id = o.order_id
            WHERE o.order_date >= CURRENT_DATE - INTERVAL '12 months'
            GROUP BY p.product_id, p.name, p.category, DATE_TRUNC('month', o.order_date)
        ),
        product_trends AS (
            SELECT
                product_id,
                product_name,
                category,
                SUM(units_sold) as total_units_12m,
                SUM(revenue) as total_revenue_12m,
                AVG(units_sold) as avg_monthly_units,
                STDDEV(units_sold) as stddev_monthly_units,
                -- ç°¡å–®ç·šæ€§è¶¨å‹¢
                CORR(EXTRACT(EPOCH FROM month), units_sold) as trend_correlation
            FROM product_monthly_sales
            GROUP BY product_id, product_name, category
        )
        SELECT
            product_name,
            category,
            total_units_12m,
            total_revenue_12m,
            avg_monthly_units,
            ROUND(avg_monthly_units * 1.1, 0) as forecasted_next_month,  -- å‡è¨­10%æˆé•·
            CASE
                WHEN trend_correlation > 0.7 THEN 'Strong Upward'
                WHEN trend_correlation > 0.3 THEN 'Moderate Upward'
                WHEN trend_correlation < -0.7 THEN 'Strong Downward'
                WHEN trend_correlation < -0.3 THEN 'Moderate Downward'
                ELSE 'Stable'
            END as trend_direction
        FROM product_trends
        WHERE total_units_12m > 0
        ORDER BY total_revenue_12m DESC
        LIMIT 50
    """)

    return {
        "historical_trends": historical_sales["rows"],
        "seasonal_patterns": seasonal_analysis["rows"],
        "product_forecasts": product_forecast["rows"],
        "forecast_date": datetime.utcnow(),
        "forecast_horizon": "1_month",
        "model_type": "simple_statistical"
    }
```

## ğŸ¯ æœ€ä½³å¯¦å‹™å»ºè­°

### 1. é€£ç·šç®¡ç†
- ä½¿ç”¨é€£ç·šæ± é¿å…é »ç¹å»ºç«‹é€£ç·š
- ç‚ºä¸åŒç”¨é€”è¨­ç«‹ä¸åŒçš„é€£ç·šï¼ˆè®€å–ã€å¯«å…¥ã€åˆ†æï¼‰
- å®šæœŸæ¸¬è©¦é€£ç·šå¥åº·ç‹€æ³

### 2. æŸ¥è©¢å„ªåŒ–
- ä½¿ç”¨åƒæ•¸åŒ–æŸ¥è©¢é˜²æ­¢ SQL æ³¨å…¥
- å°å¤§é‡è³‡æ–™ä½¿ç”¨åˆ†é æˆ–æ‰¹æ¬¡è™•ç†
- åˆ©ç”¨ `explain_query` åˆ†ææŸ¥è©¢æ•ˆèƒ½

### 3. å®‰å…¨æ€§
- ç”Ÿç”¢ç’°å¢ƒå•Ÿç”¨åªè®€æ¨¡å¼
- è¨­å®šé©ç•¶çš„æ“ä½œç™½åå–®
- è¨˜éŒ„æ‰€æœ‰é‡è¦æ“ä½œçš„å¯©è¨ˆæ—¥èªŒ

### 4. ç›£æ§èˆ‡ç¶­è­·
- å»ºç«‹è‡ªå‹•åŒ–å¥åº·æª¢æŸ¥
- è¨­å®šæ•ˆèƒ½æŒ‡æ¨™è­¦å ±
- å®šæœŸåŸ·è¡Œè³‡æ–™åº«ç¶­è­·ä»»å‹™

### 5. éŒ¯èª¤è™•ç†
- ä½¿ç”¨äº‹å‹™ç¢ºä¿è³‡æ–™ä¸€è‡´æ€§
- å¯¦ç¾é‡è©¦æ©Ÿåˆ¶è™•ç†æš«æ™‚æ€§éŒ¯èª¤
- è¨˜éŒ„è©³ç´°éŒ¯èª¤æ—¥èªŒä¾¿æ–¼é™¤éŒ¯

## ğŸ“š æ›´å¤šè³‡æº

- **ğŸ“– å®Œæ•´æ–‡ä»¶**: [MCP Server ä½¿ç”¨æ‰‹å†Š](MCP_SERVER_HANDBOOK.md)
- **ğŸ”Œ å®¢æˆ¶ç«¯ç¯„ä¾‹**: [MCP å®¢æˆ¶ç«¯æ•´åˆç¯„ä¾‹](examples/MCP_CLIENT_EXAMPLES.md)
- **ğŸš€ å¿«é€Ÿé–‹å§‹**: [å¿«é€Ÿé–‹å§‹æŒ‡å—](../QUICK_START.md)
- **ğŸ³ Docker éƒ¨ç½²**: [Docker Hub æŒ‡å—](DOCKER_HUB_GUIDE.md)
- **â“ å¸¸è¦‹å•é¡Œ**: [FAQ æ–‡ä»¶](../QA.md)

---

> **ğŸ’¡ æç¤º**: é€™äº›ä½¿ç”¨å ´æ™¯å±•ç¤ºäº† PostgreSQL MCP Server çš„å¼·å¤§åŠŸèƒ½å’Œéˆæ´»æ€§ã€‚ä½ å¯ä»¥æ ¹æ“šè‡ªå·±çš„æ¥­å‹™éœ€æ±‚ï¼Œèª¿æ•´å’Œçµ„åˆé€™äº›ç¯„ä¾‹ä¾†è§£æ±ºå¯¦éš›å•é¡Œã€‚å»ºè­°å¾ç°¡å–®å ´æ™¯é–‹å§‹ï¼Œé€æ­¥æ“´å±•åˆ°è¤‡é›œçš„æ‡‰ç”¨ã€‚